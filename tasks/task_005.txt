# Task ID: 5
# Title: LangChain Issue Classifier Service
# Status: pending
# Dependencies: 3, 4
# Priority: high
# Description: Develop the worker service that consumes raw issues from Kafka, uses LangChain with OpenAI to classify and enrich them, and stores results in Postgres.
# Details:
1. Create Python service in `/classifier` directory
2. Implement Kafka consumer for 'issues.raw' topic using aiokafka:
```python
from aiokafka import AIOKafkaConsumer
import json

consumer = AIOKafkaConsumer(
    'issues.raw',
    bootstrap_servers='redpanda:9092',
    group_id='issue-classifier',
    value_deserializer=lambda m: json.loads(m.decode('utf-8'))
)
```
3. Create `IssueClassifier` class using LangChain (v0.1.0+) with OpenAI GPT-4o:
```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

class ClassificationResult(BaseModel):
    component: str = Field(description="The technical component this issue relates to")
    severity: str = Field(description="Severity level: critical, high, medium, or low")
    summary: str = Field(description="A concise summary of the issue")
    tags: list[str] = Field(description="Relevant tags for this issue")

class IssueClassifier:
    def __init__(self, openai_api_key):
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0, api_key=openai_api_key)
        self.parser = PydanticOutputParser(pydantic_object=ClassificationResult)
        self.prompt = ChatPromptTemplate.from_template(
            """Classify the following GitHub issue:
            Title: {title}
            Body: {body}
            
            Determine the technical component, severity, provide a summary, and suggest tags.
            {format_instructions}
            """
        )
        
    async def classify(self, title, body):
        formatted_prompt = self.prompt.format(
            title=title,
            body=body,
            format_instructions=self.parser.get_format_instructions()
        )
        response = await self.llm.ainvoke(formatted_prompt)
        return self.parser.parse(response.content)
        
    async def get_embedding(self, text):
        # Use OpenAI embeddings API
        # Return 1536-dimension vector
```
4. Implement database operations to store enriched issues
5. Add producer for 'issues.enriched' topic
6. Implement rate limiting and token usage tracking for OpenAI API
7. Add error handling, retries, and dead-letter queue for failed processing
8. Configure worker pool with configurable concurrency

# Test Strategy:
1. Unit tests with mocked LLM responses
2. Integration tests with sample GitHub issues
3. Measure classification accuracy against human-labeled test set
4. Benchmark processing throughput and latency
5. Test error handling with malformed inputs
6. Validate OpenAI API rate limiting behavior
